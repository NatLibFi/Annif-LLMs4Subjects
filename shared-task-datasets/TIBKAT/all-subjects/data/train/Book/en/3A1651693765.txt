Novelty, Information and Surprise
Part I Surprise and Information of Descriptions: Prerequisites -- Improbability and Novelty of Descriptions -- Conditional Novelty and Information -- Part II Coding and Information Transmission: On Guessing and Coding -- Information Transmission -- Part III Information Rate and Channel Capacity: Stationary Processes and Information Rate -- Channel Capacity -- Shannon's Theorem -- Part IV Repertoires and Covers: Repertoires and Descriptions -- Novelty, Information and Surprise of Repertoires -- Conditioning, Mutual Information and Information Gain -- Part V Information, Novelty and Surprise in Science: Information, Novelty and Surprise in Brain Theory -- Surprise from Repetitions and Combination of Surprises -- Entropy in Physics -- Part VI Generalized Information Theory: Order- and Lattice-Structures -- Three Orderings on Repertoires -- Information Theory on Lattices of Covers -- Appendices: A. Fuzzy Repertoires and Descriptions -- A.1 Basic Definitions -- A.2 Definition and Properties of Fuzzy Repertoires -- Glossary -- Bibliography -- Index.
The book offers a new approach to information theory that is more general then the classical approach by Shannon. The classical definition of information is given for an alphabet of symbols or for a set of mutually exclusive propositions (a partition of the probability space Ω) with corresponding probabilities adding up to 1. The new definition is given for an arbitrary cover of Ω, i.e. for a set of possibly overlapping propositions. The generalized information concept is called novelty and it is accompanied by two new concepts derived from it, designated as information and surprise, which describe "opposite" versions of novelty, information being related more to classical information theory and surprise being related more to the classical concept of statistical significance. In the discussion of these three concepts and their interrelations several properties or classes of covers are defined, which turn out to be lattices. The book also presents applications of these new concepts, mostly in statistics and in neuroscience.
