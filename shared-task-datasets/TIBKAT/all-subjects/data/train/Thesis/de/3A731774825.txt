Physische Datenbankoptimierung in hauptspeicherbasierten Column-Store-Systemen
Im heutigen Informationszeitalter ist es für Unternehmen entscheidend, möglichst schnell auf sehr große Datenmengen zuzugreifen und komplexe Berechnungen effizient darauf ausführen zu können. Weil Anfragen zur Entscheidungsunterstützung in den meisten Fällen nur einen Bruchteil aller Spalten benötigen, hat sich in diesem Bereich das Konzept der spaltenweisen Datenorganisation als vorteilhaft erwiesen. Bedingt durch die anfänglich geringen Einsatzmöglichkeiten und den nur begrenzt verfügbaren Hauptspeicher wurde das physische Design von hauptspeicherbasierten Column-Store-Systemen jedoch lange Zeit nicht tiefergehend untersucht. Das Ziel dieser Dissertation besteht nun darin, den Einsatz von leichtgewichtigen Kompressionstechniken in Column Stores zur effizienten Speicherung von Massendaten und zur Beschleunigung von Anfragen zu untersuchen und Empfehlungen für den optimalen Einsatz zu geben. Als leichtgewichtige Kompressionstechniken werden in dieser Arbeit Verfahren bezeichnet, bei denen für die Dekompression wenige Prozessorzyklen und kaum zusätzlicher Hauptspeicher erforderlich ist. Um trotzdem eine hohe Kompressionsrate zu erzielen, werden mehrere Heuristiken vorgestellt, die mit Hilfe von Informationen über die Datenverteilung und Abhängigkeiten zwischen Spalten die Daten geeignet umsortieren. Für die effiziente Bestimmung der funktionalen Abhängigkeiten wird ein auf Sampling basierendes Verfahren vorgestellt, das statt der Häufigkeit von Werten und Wertepaaren die Entropie als Maß verwendet und damit auch bei einer kleinen Stichprobe zuverlässig funktioniert. Die ermittelten Informationen werden weiterhin dazu genutzt, abhängige Spalten zu kombinieren und dadurch den Speicherplatzbedarf weiter zu reduzieren. Neben einer effektiven Kompression ist für eine schnelle Anfrageausführung auch die Anpassung von häufig verwendeten Anfrageoperatoren, wie dem Scan oder die Aggregation, notwendig.Die vorgestellten Algorithmen operieren direkt auf den komprimierten Datenstrukturen und vermeiden damit eine teure Dekompression von Werten. Zusätzlich zu den beschriebenen Software-Optimierungen wird auch ein Überblick über Verfahren gegeben, die moderne Hardware zur Beschleunigung von Anfragen ausnutzen.Abschließend werden alle entwickelten Heuristiken und Algorithmen in ein bestehendes Produkt (\ice) integriert und detailliert ausgewertet. Dabei wird deutlich, dass Anfragen mit bestimmten Zugriffsmustern auf komprimierten Daten langsamer sind und nur durch neue, angepasste Algorithmen schnell ausgeführt werden können.
