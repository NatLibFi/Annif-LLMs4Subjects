Data parallel C++ : programming accelerated systems Using C++ and SYCL
"This book, now in is second edition, is the premier resource to learn SYCL 2020 and is the ONLY book you need to become part of this community." Erik Lindahl, GROMACS and Stockholm University Learn how to accelerate C++ programs using data parallelism and SYCL. This open access book enables C++ programmers to be at the forefront of this exciting and important development that is helping to push computing to new levels. This updated second edition is full of practical advice, detailed explanations, and code examples to illustrate key topics. SYCL enables access to parallel resources in modern accelerated heterogeneous systems. Now, a single C++ application can use any combination of devices–including GPUs, CPUs, FPGAs, and ASICs–that are suitable to the problems at hand. This book teaches data-parallel programming using C++ with SYCL and walks through everything needed to program accelerated systems. The book begins by introducing data parallelism and foundational topics for effective use of SYCL. Later chapters cover advanced topics, including error handling, hardware-specific programming, communication and synchronization, and memory model considerations. All source code for the examples used in this book is freely available on GitHub. The examples are written in modern SYCL and are regularly updated to ensure compatibility with multiple compilers. You Will Learn How to: Accelerate C++ programs using data-parallel programming Use SYCL and C++ compilers that support SYCL Write portable code for accelerators that is vendor and device agnostic Optimize code to improve performance for specific accelerators Be poised to benefit as new accelerators appear from many vendors.
Chapter 1: Introduction -- Chapter 2: Where Code Executes -- Chapter 3: Data Management and Ordering the Uses of Data -- Chapter 4: Expressing Parallelism -- Chapter 5: Error Handling -- Chapter 6: Unified Shared Memory -- Chapter 7: Buffers -- Chapter 8: Scheduling Kernels and Data Movement -- Chapter 9: Local Memory and Work-group Barriers -- Chapter 10: Defining Kernels -- Chapter 11: Vector and Math Arrays -- Chapter 12: Device Information and Kernel Specialization -- Chapter 13: Practical Tips -- Chapter 14: Common Parallel Patterns -- Chapter 15: Programming for GPUs -- Chapter 16: Programming for CPUs -- Chapter 17: Programming for FFGAs -- Chapter 18: Libraries -- Chapter 19: Memory Model and Atomics -- Chapter 20: Backend Interoperability -- Chapter 21: Migrating CUDA Code -- Epilogue.
