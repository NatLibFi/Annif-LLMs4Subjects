Advanced analytics with PySpark : patterns for learning from data at scale using Python and Spark
Analyzing big data -- Introduction to data analysis with PySpark -- Recommending music and the audioscrobbler dataset -- Making predictions with decision trees and decision forests -- Anomaly detection with K-means clustering -- Understanding Wikipedia with LDA and Spark NLP -- Geospatial and temporal data analysis on taxi trip data -- Estimating financial risk -- Analyzing genomics data and the BDG project -- Image similarity detection with deep learning and PySpark LSH -- Managing the machine learning lifecycle with MLflow.
The amount of data being generated today is staggering-- and growing.  Apache Spark has emerged as the de facto tool for analyzing big data and is now a critical part of the data science toolbox.  Updated for Spark 3.0, this practical guide brings together Spark, statistical methocs, and real-world datasets to teach you how to approach analytics problems using PySpark, Spark's Python API, and other best practices in Spark programming.  Data scientists Akash Tandon, Sandy Ryza, Uri Laserson, Sean Owen, and Josh Wills offer an introduction to the Spark ecosystem, then dive into patterns that apply common techniques-- including classification, clustering, collaborative filtering, and anomaly detection-- to fields such as genomics, security, and finance.  This updated edition also covers image processing and the Spark NLP library
