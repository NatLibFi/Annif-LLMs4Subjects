Deep learning with PyTorch : a practical approach to building neural network models using PyTorch
This book provides the intuition behind the state of the art Deep Learning architectures such as ResNet, DenseNet, Inception, and encoder-decoder without diving deep into the math of it. It shows how you can implement and use various architectures to solve problems in the area of image classification, language translation and NLP using PyTorch.
Generate batches of vectors -- Creating a network model with embedding -- Training the model -- Using pretrained word embeddings -- Downloading the embeddings -- Loading the embeddings in the model -- Freeze the embedding layer weights -- Recursive neural networks -- Understanding how RNN works with an exampleÂ -- LSTM -- Long-term dependencyÂ -- LSTM networks -- Preparing the data -- Creating batches -- Creating the networkÂ -- Training the model -- Convolutional network on sequence data -- Understanding one-dimensional convolution for sequence dataÂ -- Creating the network -- Training the model -- Summary -- Chapter 7: Generative Networks -- Neural style transferÂ -- Loading the data -- Creating the VGG model -- Content loss -- Style loss -- Extracting the lossesÂ -- Creating loss function for each layers -- Creating the optimizer -- Training -- Generative adversarial networks -- Deep convolutional GAN -- Defining the generator network -- Transposed convolutions -- Batch normalization -- Generator -- Defining the discriminator network -- Defining loss and optimizer -- Training the discriminator -- Training the discriminator with real images -- Training the discriminator with fake images -- Training the generator networkÂ -- Training the complete network -- Inspecting the generated images -- Language modelingÂ -- Preparing the dataÂ -- Generating the batchesÂ -- BatchesÂ -- Backpropagation through time -- Defining a model based on LSTM -- Defining the train and evaluate functions -- Training the model -- Summary -- Chapter 8: Modern Network Architectures -- Modern network architectures -- ResNetÂ -- Creating PyTorch datasets -- Creating loaders for training and validation -- Creating a ResNet model -- Extracting convolutional features -- Creating a custom PyTorch dataset class for the pre-convoluted features and loader
Creating a simple linear model -- Training and validating the model -- Inception -- Creating an Inception model -- Extracting convolutional features using register_forward_hook -- Creating a new dataset for the convoluted features -- Creating a fully connected model -- Training and validating the model -- Densely connected convolutional networks â DenseNet -- DenseBlock -- DenseLayer -- Creating a DenseNet model -- Extracting DenseNet features -- Creating a dataset and loaders -- Creating a fully connected model and train -- Model ensembling -- Creating models -- Extracting the image features -- Creating a custom dataset along with data loaders -- Creating an ensembling model -- Training and validating the model -- Encoder-decoder architectureÂ -- EncoderÂ -- Decoder -- Summary -- Chapter 9: What Next? -- What next? -- Overview -- Interesting ideas to explore -- Object detection -- Image segmentation -- OpenNMT in PyTorch -- Alien NLP -- fast.ai â making neural nets uncool again -- Open Neural Network ExchangeÂ -- How to keep yourself updated -- Summary -- Other Books You May Enjoy -- Index
Cover -- Copyright and Credits -- Dedication -- Packt Upsell -- Foreword -- Contributors -- Table of Contents -- Preface -- Chapter 1: Getting Started with Deep Learning Using PyTorch -- Artificial intelligence -- The history of AI -- Machine learning -- Examples of machine learning in real life -- Deep learning -- Applications of deep learning -- Hype associated with deep learningÂ -- The history of deep learningÂ -- Why now? -- Hardware availability -- Data and algorithms -- Deep learning frameworks -- PyTorch -- Summary -- Chapter 2: Building Blocks of Neural Networks -- Installing PyTorch -- Our first neural network -- Data preparation -- Scalar (0-D tensors)Â -- Vectors (1-D tensors) -- Matrix (2-D tensors) -- 3-D tensors -- Slicing tensorsÂ -- 4-D tensors -- 5-D tensors -- Tensors on GPU -- Variables -- Creating data for our neural network -- Creating learnable parameters -- Neural network model -- Network implementationÂ -- Loss function -- Optimize the neural networkÂ -- Loading dataÂ -- Dataset class -- DataLoader class -- SummaryÂ -- Chapter 3: Diving Deep into Neural Networks -- Deep dive into the building blocks of neural networks -- Layers â fundamental blocks of neural networks -- Non-linear activations -- Sigmoid -- Tanh -- ReLU -- Leaky ReLU -- PyTorch non-linear activations -- The PyTorch way of building deep learning algorithms -- Model architecture for different machine learning problems -- Loss functions -- Optimizing network architecture -- Image classification using deep learning -- Loading data into PyTorch tensors -- Loading PyTorch tensors as batches -- Building the network architecture -- Training the modelÂ -- Summary -- Chapter 4: Fundamentals of Machine Learning -- Three kinds of machine learning problems -- Supervised learning -- Unsupervised learning -- Reinforcement learning -- Machine learning glossary.
Evaluating machine learning models -- Training, validation, and test split -- Simple holdout validation -- K-fold validation -- K-fold validation with shufflingÂ -- Data representativenessÂ -- Time sensitivity -- Data redundancy -- Data preprocessing and feature engineering -- Vectorization -- Value normalization -- Handling missing values -- Feature engineering -- Overfitting and underfitting -- Getting more data -- Reducing the size of the network -- Applying weight regularization -- Dropout -- Underfitting -- Workflow of a machine learning project -- Problem definition and dataset creation -- Measure of successÂ -- Evaluation protocol -- Prepare your data -- Baseline model -- Large model enough to overfit -- Applying regularization -- Learning rate picking strategiesÂ -- Summary -- Chapter 5: Deep Learning for Computer Vision -- Introduction to neural networks -- MNIST â getting data -- Building a CNN model from scratch -- Conv2d -- Pooling -- Nonlinear activation â ReLU -- View -- Linear layer -- Training the model -- Classifying dogs and cats â CNN from scratch -- Classifying dogs and cats using transfer learning -- Creating and exploring a VGG16 modelÂ -- Freezing the layers -- Fine-tuning VGG16 -- Training the VGG16 modelÂ -- Calculating pre-convoluted features -- Understanding what a CNN model learnsÂ -- Visualizing outputs from intermediate layers -- Visualizing weights of the CNN layer -- Summary -- Chapter 6: Deep Learning with Sequence Data and Text -- Working with text data -- Tokenization -- Converting text into characters -- Converting text into words -- N-gram representation -- Vectorization -- One-hot encoding -- Word embedding -- Training word embeddingÂ by building a sentiment classifier -- Downloading IMDB data and performing text tokenization -- torchtext.data -- torchtext.datasets -- Building vocabulary
