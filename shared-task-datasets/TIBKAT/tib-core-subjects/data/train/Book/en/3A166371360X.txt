Data science with Python and Dask
An efficient data pipeline means everything for the success of a data science project. Dask is a flexible library for parallel computing in Python that makes it easy to build intuitive workflows for ingesting and analyzing large, distributed datasets. Dask provides dynamic task scheduling and parallel collections that extend the functionality of NumPy, Pandas, and Scikit-learn, enabling users to scale their code from a single laptop to a cluster of hundreds of machines with ease. "Data science with Python and Dask" teaches you to build scalable projects that can handle massive datasets. After meeting the Dask framework, you'll analyze data in the nYC Parking Ticket database and use DataFrames to streamline your process. Then, you'll create machine learning models using Dask-ML, build interactive visualizations, and build clusters using AWS and Docker
Part 1. The building blocks of scalable computing. Why scalable computing matters -- Introducing Dask -- Part 2. Working with structured data using Dask DataFrames. Introducing Dask DataFrames -- Loading data into DataFrames -- Cleaning and transforming DataFrames -- Summarizing and analyzing DataFrames -- Visualizing DataFrames with Seaborn -- Visualizing location data with Datashader -- Part 3. Extending and deploying Dask. Working with bags and arrays -- Machine learning with Dask-ML -- Scaling and deploying Dask.
