Statistical Field Theory for Neural Networks
Introduction -- Probabilities, moments, cumulants -- Gaussian distribution and Wickâ€™s theorem -- Perturbation expansion -- Linked cluster theorem -- Functional preliminaries -- Functional formulation of stochastic differential equations -- Ornstein-Uhlenbeck process: The free Gaussian theory -- Perturbation theory for stochastic differential equations -- Dynamic mean-field theory for random networks -- Vertex generating function -- Application: TAP approximation -- Expansion of cumulants into tree diagrams of vertex functions -- Loopwise expansion of the effective action - Tree level -- Loopwise expansion in the MSRDJ formalism -- Nomenclature.
This book presents a self-contained introduction to techniques from field theory applied to stochastic and collective dynamics in neuronal networks. These powerful analytical techniques, which are well established in other fields of physics, are the basis of current developments and offer solutions to pressing open problems in theoretical neuroscience and also machine learning. They enable a systematic and quantitative understanding of the dynamics in recurrent and stochastic neuronal networks. This book is intended for physicists, mathematicians, and computer scientists and it is designed for self-study by researchers who want to enter the field or as the main text for a one semester course at advanced undergraduate or graduate level. The theoretical concepts presented in this book are systematically developed from the very beginning, which only requires basic knowledge of analysis and linear algebra.
