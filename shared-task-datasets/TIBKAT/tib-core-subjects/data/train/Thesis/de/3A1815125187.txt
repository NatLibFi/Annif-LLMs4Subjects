Adversarials-1 : detecting adversarial inputs with internal attacks
Tiefe neuronale Netze sind sehr erfolgreich bei verschiedenen anspruchsvollen Aufgaben, z.B. bei der Bild- und Sprachklassifikation. Dennoch sind sie anfällig gegenüber Angriffen bei denen die Eingabe leicht verändert wird, was zu einer Fehlklassifikation führt. In dieser Arbeit wird zunächst ein neues Angriffsszenario eingeführt um solche Angriffe gegen Straßenschilder ohne physische Manipulation durchzuführen. Danach wird eine Abwehrstrategie vorgestellt, deren Grundidee es ist eine unbekannte Eingabe intern zu manipulieren. Auf Basis der interen Manipulation wird entschieden ob die initiale Eingabe originär oder bereits manipuliert war. Im zweiten Fall kann durch die interne Manipulation die originale Klasse wiederhergestellt werden. Durch Experimente wird gezeigt, dass dieses Verfahren sowohl in der Bild- als auch Sprachklassifikation angewendet werden kann. Zuletzt wird gezeigt, dass das Verfahren auch verwendet werden kann um allgemeinere out-of-distribution Eingaben zu erkennen.
Deep neural networks are very successful in various demanding tasks, e.g. in image and speech classification. Nevertheless, they are vulnerable to attacks where the input is slightly modified, which leads to misclassification. In this thesis a new attack scenario is introduced to perform such attacks against road signs without physical manipulation. Then a defence strategy is presented, whose basic idea is to manipulate an unknown input internally. Based on the internal manipulation it is decided whether the initial input was original or already manipulated. In the second case the original class can be restored by the internal manipulation. Experiments show that this procedure can be applied in both image and speech classification. Finally, it is shown that the method can also be used to detect more general out-of-distribution input.
