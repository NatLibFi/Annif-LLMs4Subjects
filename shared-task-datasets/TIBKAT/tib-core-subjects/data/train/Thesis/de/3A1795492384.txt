Non-Linear Latent Variable Models for Inference and Learning from Non-Gaussian Data
Wir schlagen eine Familie von probabilistischen generativen Modellen vor, die eine Vielzahl von Wahrscheinlichkeitsverteilungen aus der exponentiellen Familie umfassen. Darüber hinaus untersuchen wir eine punktweise Maximumfunktion und führen, für 2-parametrige Rauschverteilungen, eine neuartige nichtlineare Superposition zur Kopplung der Latenten und Observablen mit zwei Matrizen ein: Eine zur Modellierung der Komponentenmittelwerte und eine andere für Komponentenvarianzen. Wir nutzen den EM Algorithmus und zeigen, dass die vorgestellte Linkfunktion die Herleitung eines sehr allgemeinen und präzisen Satzes von Parameteraktualisierungsgleichungen ermöglicht. Wir leiten eine Reihe von Gleichungen her, die für alle regulären Verteilungen der Exponentialfamilie dieselbe funktionale Form haben. Unsere Ergebnisse liefern direkt anwendbare Lerngleichungen sowohl für gewöhnlich als auch für ungewöhnlich verteilte Daten. Wir betrachten außerdem verschiedene Anwendungen der vorgeschlagenen Modelle und untersuchen eine Vielzahl komplexer Datensätze, die sowohl synthetische als auch reale Daten umfassen.
We present a family of probabilistic generative models that encompasses a variety of probability distributions (including Gaussian, Gamma, Beta, Poisson and many more) from the exponential family. In addition, we investigate a point-wise maximum function and introduce a novel non-linear superposition for coupling the latents and observables using two matrices (if the considered noise distribution has two parameters): One to model the component means and another for component variances. We further exploit the Expectation Maximization (EM) algorithm and show that the presented link function allows for the derivation of a very general and concise set of parameter update equations. Concretely, we derive a set of updates that have the same functional form for all regular distributions of the exponential family. Our results then provide directly applicable learning equations for commonly as well as for unusually distributed data. Finally, to assess the reliability of our theoretical findings, we consider different applications of the proposed generative models and investigate a variety of complex datasets including both synthetic and real data.
