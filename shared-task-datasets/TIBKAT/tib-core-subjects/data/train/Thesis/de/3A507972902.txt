Methoden der vision-basierten Nutzerwahrnehmung für eine natürliche Interaktion mit mobilen Servicerobotern
Methoden der vision-basierten Nutzerwahrnehmung für eine natürliche Interaktion mit mobilen Servicerobotern Im Gegensatz zur zwischenmenschlichen Kommunikation, bei der die Beziehungsebene im Vergleich zur Sachebene den weitaus größeren Anteil einnimmt, wird diese bei der Mensch-Roboter-Interaktion bislang nur in Ansätzen berücksichtigt. Insbesondere die Nutzerwahrnehmung bleibt in der Regel auf eine reine Personendetektion oder ein einfaches Personen-Tracking beschränkt. Vor diesem Hintergrund wurde eine verbesserte Wahrnehmung des aktuellen Zustandes des Nutzers als Voraussetzung für eine Personalisierung des Dialogs als Zielstellung dieser Arbeit abgeleitet. Beim exemplarischen Anwendungsszenario handelt es sich um einen Shopping-Assistenten, der in einem Baumarkt den Kunden bei der Suche nach Produkten behilflich ist. Dieser sollte zumindest einen gewissen Grad an sozialer Kompetenz zeigen, indem er z.B. Personen in seiner Umgebung detektiert und während der Interaktion kontinuierlich Blickkontakt hält. Um Nutzermodelle erstellen, kurzzeitig verlorene Nutzer wiedererkennen und den Gemütszustand des Nutzers abschätzen zu können, sollen Geschlecht, Alter, Identität und Gesichtsausdruck des Nutzers aus einem Videobild ermittelt werden. Für die Realisierung dieser Aufgabe wurde eine biologisch motivierte Aufteilung in ein peripheres und ein foveales Vision-System vorgeschlagen. Das periphere System arbeitet auf den Bildern einer omnidirektionalen Kamera und verfügt damit über einen sehr großen Sichtbereich, aber nur eine vergleichsweise geringe Auflösung. In diesem System werden zunächst Hypothesen über die Position von Personen im Umfeld des Roboters gebildet. Dafür werden Hautfarbe, Bewegung und Entfernung in einer Auffälligkeitskarte integriert und auffällige Bildbereiche mittels eines Multi-Target-Trackers verfolgt. Für die omnidirektionale Kamera wurde ein automatischer Weißabgleich entwickelt, der die Hautfarbdetektion unempfindlich gegen Änderungen der Chrominanz der Beleuchtung macht. Nach Auswahl einer Nutzerhypothese wird der Kopf des Roboters kontinuierlich in die entsprechende Richtung ausgerichtet. Damit erhält der Nutzer zum einen eine Rückmeldung über die gerichtete Aufmerksamkeit des Roboters während der Interaktion. Zum anderen kann der Roboter hochaufgelöste Bilder der Person aufnehmen, so dass eine weitere nachfolgende Analyse ermöglicht wird. Diese ist wiederum in zwei Teilschritte unterteilt. Der erste Schritt besteht aus einer Detektion des Gesichtes und einer anschließenden Detektion der Augen, anhand derer eine normalisierte Darstellung des Gesichtes erzeugt wird. Für den Analyseschritt wurden das Elastic-Graph-Matching, die Independent Component Analysis und die Active-Appearance Models implementiert und vergleichend untersucht. Unter Berücksichtigung der Anforderungen einer Geschlechts-, Alters-, Mimik- und Identitätsschätzung wurde hierfür eine umfassende Gesichtsdatenbank zum Training und zum Test der Verfahren angelegt. Die Leistungsfähigkeit des Gesamtsystems wurde schließlich anhand von empirischen Experimenten demonstriert.
